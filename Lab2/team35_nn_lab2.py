# -*- coding: utf-8 -*-
"""Team35_NN_lab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jbaC2KoZvS1QoKAN7qk_lDHQIyMN2BBD

# Εργαστηριακή Άσκηση 2. Μη επιβλεπόμενη μάθηση. 
## Σύστημα συστάσεων βασισμένο στο περιεχόμενο
## Σημασιολογική απεικόνιση δεδομένων με χρήση SOM 
Ημερομηνία εκφώνησης της άσκησης: 23 Νοεμβρίου 2020

#Στοιχεία Ομάδας
##Ομάδα 35

#### - Καλτσογιάννης Δημήτρης       *03115628*
#### - Μάμαλη Αικατερίνη            *03116017*
#### - Τριανταφυλλόπουλος Ηλίας     *03116028*
"""

!pip install --upgrade pip
!pip install --upgrade numpy
!pip install --upgrade pandas
!pip install --upgrade nltk
!pip install --upgrade scikit-learn
!pip install --upgrade joblib
!pip install --upgrade somoclu

#getting saved data
import joblib
import somoclu
som10 = joblib.load('som10.pkl')
som20 = joblib.load('som20.pkl')
som25 = joblib.load('som25.pkl')
som30 = joblib.load('som30.pkl')
final_corpus = joblib.load('final_corpus.pkl')
corpus_tf_idf = joblib.load('corpus_tf_idf.pkl')

"""## Εισαγωγή του Dataset

Το σύνολο δεδομένων με το οποίο θα δουλέψουμε είναι βασισμένο στο [Carnegie Mellon Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/). Πρόκειται για ένα dataset με περίπου 40.000 περιγραφές ταινιών. Η περιγραφή κάθε ταινίας αποτελείται από τον τίτλο της, μια ή περισσότερες ετικέτες που χαρακτηρίζουν το είδος της ταινίας και τέλος τη σύνοψη της υπόθεσής της. Αρχικά εισάγουμε το dataset (χρησιμοποιήστε αυτούσιο τον κώδικα, δεν χρειάζεστε το αρχείο csv) στο dataframe `df_data_1`:
"""

import pandas as pd

dataset_url = "https://drive.google.com/uc?export=download&id=1PdkVDENX12tQliCk_HtUnAUbfxXvnWuG"
df_data_1 = pd.read_csv(dataset_url, sep='\t',  header=None, quoting=3, error_bad_lines=False)

"""Κάθε ομάδα θα δουλέψει σε ένα μοναδικό υποσύνολο 5.000 ταινιών (διαφορετικό dataset για κάθε ομάδα) ως εξής

1. Κάθε ομάδα έχει έναν αριθμό "seed" (φύτρο) που είναι ο ίδιος με τον αριθμό της ομάδας σας: θα τον βρείτε στην κολόνα Α/Α [εδώ](https://docs.google.com/spreadsheets/d/1CD6AtX7YnocXceCELl_XJ06kyRr0YQPhor8dpw012t0/edit?usp=sharing).

2. Το data frame `df_data_2` έχει γραμμές όσες και οι ομάδες και 5.000 στήλες. Σε κάθε ομάδα αντιστοιχεί η γραμμή του πίνακα με το `team_seed_number` της. Η γραμμή αυτή θα περιλαμβάνει 5.000 διαφορετικούς αριθμούς που αντιστοιχούν σε ταινίες του αρχικού dataset. 

3. Στο επόμενο κελί αλλάξτε τη μεταβλητή `team_seed_number` με το Seed της ομάδας σας.

4. Τρέξτε τον κώδικα. Θα προκύψουν τα μοναδικά για κάθε ομάδα  titles, categories, catbins, summaries και corpus με τα οποία θα δουλέψετε.
"""

import numpy as np

# βάλτε το seed που αντιστοιχεί στην ομάδα σας
team_seed_number = 35

movie_seeds_url = "https://drive.google.com/uc?export=download&id=1EA_pUIgK5Ub3kEzFbFl8wSRqAV6feHqD"
df_data_2 = pd.read_csv(movie_seeds_url, header=None, error_bad_lines=False)

# επιλέγεται 
my_index = df_data_2.iloc[team_seed_number,:].values

titles = df_data_1.iloc[:, [2]].values[my_index] # movie titles (string)
categories = df_data_1.iloc[:, [3]].values[my_index] # movie categories (string)
bins = df_data_1.iloc[:, [4]]
catbins = bins[4].str.split(',', expand=True).values.astype(np.float)[my_index] # movie categories in binary form (1 feature per category)
summaries =  df_data_1.iloc[:, [5]].values[my_index] # movie summaries (string)
corpus = summaries[:,0].tolist() # list form of summaries

"""- Ο πίνακας **titles** περιέχει τους τίτλους των ταινιών. Παράδειγμα: 'Sid and Nancy'.
- O πίνακας **categories** περιέχει τις κατηγορίες (είδη) της ταινίας υπό τη μορφή string. Παράδειγμα: '"Tragedy",  "Indie",  "Punk rock",  "Addiction Drama",  "Cult",  "Musical",  "Drama",  "Biopic \[feature\]",  "Romantic drama",  "Romance Film",  "Biographical film"'. Παρατηρούμε ότι είναι μια comma separated λίστα strings, με κάθε string να είναι μια κατηγορία.
- Ο πίνακας **catbins** περιλαμβάνει πάλι τις κατηγορίες των ταινιών αλλά σε δυαδική μορφή ([one hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)). Έχει διαστάσεις 5.000 x 322 (όσες οι διαφορετικές κατηγορίες). Αν η ταινία ανήκει στο συγκεκριμένο είδος η αντίστοιχη στήλη παίρνει την τιμή 1, αλλιώς παίρνει την τιμή 0.
- Ο πίνακας **summaries** και η λίστα **corpus** περιλαμβάνουν τις συνόψεις των ταινιών (η corpus είναι απλά ο summaries σε μορφή λίστας). Κάθε σύνοψη είναι ένα (συνήθως μεγάλο) string. Παράδειγμα: *'The film is based on the real story of a Soviet Internal Troops soldier who killed his entire unit  as a result of Dedovschina. The plot unfolds mostly on board of the prisoner transport rail car guarded by a unit of paramilitary conscripts.'*
- Θεωρούμε ως **ID** της κάθε ταινίας τον αριθμό γραμμής της ή το αντίστοιχο στοιχείο της λίστας. Παράδειγμα: για να τυπώσουμε τη σύνοψη της ταινίας με `ID=99` (την εκατοστή) θα γράψουμε `print(corpus[99])`.
"""

ID = 99
print(titles[ID])
print(categories[ID])
print(catbins[ID])
print(corpus[ID])

"""# Εφαρμογή 1. Υλοποίηση συστήματος συστάσεων ταινιών βασισμένο στο περιεχόμενο
<img src="http://clture.org/wp-content/uploads/2015/12/Netflix-Streaming-End-of-Year-Posts.jpg" width="70%">

Η πρώτη εφαρμογή που θα αναπτύξετε θα είναι ένα [σύστημα συστάσεων](https://en.wikipedia.org/wiki/Recommender_system) ταινιών βασισμένο στο περιεχόμενο (content based recommender system). Τα συστήματα συστάσεων στοχεύουν στο να προτείνουν αυτόματα στο χρήστη αντικείμενα από μια συλλογή τα οποία ιδανικά θέλουμε να βρει ενδιαφέροντα ο χρήστης. Η κατηγοριοποίηση των συστημάτων συστάσεων βασίζεται στο πώς γίνεται η επιλογή (filtering) των συστηνόμενων αντικειμένων. Οι δύο κύριες κατηγορίες είναι η συνεργατική διήθηση (collaborative filtering) όπου το σύστημα προτείνει στο χρήστη αντικείμενα που έχουν αξιολογηθεί θετικά από χρήστες που έχουν παρόμοιο με αυτόν ιστορικό αξιολογήσεων και η διήθηση με βάση το περιεχόμενο (content based filtering), όπου προτείνονται στο χρήστη αντικείμενα με παρόμοιο περιεχόμενο (με βάση κάποια χαρακτηριστικά) με αυτά που έχει προηγουμένως αξιολογήσει θετικά.

Το σύστημα συστάσεων που θα αναπτύξετε θα βασίζεται στο **περιεχόμενο** και συγκεκριμένα στις συνόψεις των ταινιών (corpus).

## Μετατροπή σε TFIDF

Το πρώτο βήμα θα είναι λοιπόν να μετατρέψετε το corpus σε αναπαράσταση tf-idf:
"""

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
vectorizer.fit(corpus)
corpus_tf_idf = vectorizer.transform(corpus)

"""Η συνάρτηση [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) όπως καλείται εδώ **δεν είναι βελτιστοποιημένη**. Οι επιλογές των μεθόδων και παραμέτρων της μπορεί να έχουν **δραματική επίδραση στην ποιότητα των συστάσεων** και είναι διαφορετικές για κάθε dataset. Επίσης, οι επιλογές αυτές έχουν πολύ μεγάλη επίδραση και στη **διαστατικότητα και όγκο των δεδομένων**. Η διαστατικότητα των δεδομένων με τη σειρά της θα έχει πολύ μεγάλη επίδραση στους **χρόνους εκπαίδευσης**, ιδιαίτερα στη δεύτερη εφαρμογή της άσκησης. Ανατρέξτε στα notebooks του εργαστηρίου και στο [FAQ](https://docs.google.com/document/d/1hou1gWXQuHAB7J2aV44xm_CtAWJ63q6Cu1V6OwyL_n0/edit?usp=sharing) των ασκήσεων.

"""

print(corpus_tf_idf.shape)

"""Παρατηρούμε ότι η διάσταση του corpus μας είναι πολύ μεγάλη. Κατά την διαδικασία βελτιστοποίησης που θα ακολουθήσουμε παρακάτω, θα προσπαθήσουμε τόσο να βελτιώσουμε τις προτάσεις ταινιών που το σύστημα μας εξάγει, όσο και να μειώσουμε την διάσταση του corpus για να κρατήσουμε την πιο σημαντική πληροφορία αυτού.

## Υλοποίηση του συστήματος συστάσεων

Το σύστημα συστάσεων που θα παραδώσετε θα είναι μια συνάρτηση `content_recommender` με δύο ορίσματα `target_movie` και `max_recommendations`. Στην `target_movie` περνάμε το ID μιας ταινίας-στόχου για την οποία μας ενδιαφέρει να βρούμε παρόμοιες ως προς το περιεχόμενο (τη σύνοψη) ταινίες, `max_recommendations` στο πλήθος.
Υλοποιήστε τη συνάρτηση ως εξής: 
- για την ταινία-στόχο, από το `corpus_tf_idf` υπολογίστε την [ομοιότητα συνημιτόνου](https://en.wikipedia.org/wiki/Cosine_similarity) της με όλες τις ταινίες της συλλογής σας
- με βάση την ομοιότητα συνημιτόνου που υπολογίσατε, δημιουργήστε ταξινομημένο πίνακα από το μεγαλύτερο στο μικρότερο, με τα indices (`ID`) των ταινιών. Παράδειγμα: αν η ταινία με index 1 έχει ομοιότητα συνημιτόνου με 3 ταινίες \[0.2 1 0.6\] (έχει ομοιότητα 1 με τον εαύτό της) ο ταξινομημένος αυτός πίνακας indices θα είναι \[1 2 0\].
- Για την ταινία-στόχο εκτυπώστε: id, τίτλο, σύνοψη, κατηγορίες (categories)
- Για τις `max_recommendations` ταινίες (πλην της ίδιας της ταινίας-στόχου που έχει cosine similarity 1 με τον εαυτό της) με τη μεγαλύτερη ομοιότητα συνημιτόνου (σε φθίνουσα σειρά), τυπώστε σειρά σύστασης (1 πιο κοντινή, 2 η δεύτερη πιο κοντινή κλπ), id, τίτλο, σύνοψη, κατηγορίες (categories)
"""

import math
import scipy as sp
import numpy as np

def content_recommender(target_movie, max_recommendations):

  target_movie_tfidf = corpus_tf_idf[target_movie]
  tm_magnitube = math.sqrt(target_movie_tfidf.power(2).sum())
  #cosine similarity evaluation
  #for vectors A, B -> cos() = A*B / |A|*|B|
  cosine_sim = []
  for b in corpus_tf_idf:
    #use round because for some reason probably connected to sqrt, we woulb notice that cos() of target_movie when compared to itself was
    #1.0000000000000009 which is not the expected result
    cosine_sim.append(round(target_movie_tfidf.multiply(b).sum() / (tm_magnitube*math.sqrt(b.power(2).sum())), 10))
  
  #sorting the list and keeping the indexes of max_recommendations(quantity) more similar movies
  similar_movies = np.argsort(cosine_sim)[-(max_recommendations+1):]

  #printing results
  print("Target Movie ID:", target_movie)
  print("Title:", titles[target_movie][0])
  print("Corpus:", corpus[target_movie])
  print("Categories:", categories[target_movie][0]) 
  print("---------------------------------------")

  for i in range(1, max_recommendations+1):
    ID = similar_movies[max_recommendations-i]
    print("Order:", i)
    print("Target Movie ID:", ID)
    print("Title:", titles[ID][0])
    print("Corpus:", corpus[ID])
    print("Categories:", categories[ID][0])
    print("------------------------------------")

content_recommender(99, 5)

"""Χρησιμοποιούμε αρχικά το σύνολο του Dataset μας και δημιουργούμε την βασική μας συνάρτηση που θα επιστρέφει τα αποτελέσματα του συστήματος συστάσεων. Έχουμε μία naive υλοποίηση του TfidfVectorizer και ομοιότητα συνημιτόνου για την εξαγωγή συμπερασμάτων. 

Επικεντρωνόμαστε στο παραπάνω παράδειγμα της 100ης ταινίας του dataset μας. Κρίνοντας από τις κατηγορίες που ανήκει η συγκεκριμένη ταινία και τα αποτελέσματα του συστήματος, δεν βλέπουμε αρκετή ομοιότητα. Αυτό πιθανώς οφείλεται σε κάποια ομοιότητα που βρέθηκε μέσα στην περιγραφή των ταινιών, η οποία έχει ως στόχο της κάποιες κοινές λέξεις ή και φράσεις που δεν θα έπρεπε να αποτελούν δείκτες ομοιότητας. Επομένως, η βελτιστοποίηση κρίνεται απαραίτητη.

## Βελτιστοποίηση

Αφού υλοποιήσετε τη συνάρτηση `content_recommender` χρησιμοποιήστε τη για να βελτιστοποιήσετε την `TfidfVectorizer`. Συγκεκριμένα, αρχικά μπορείτε να δείτε τι επιστρέφει το σύστημα για τυχαίες ταινίες-στόχους και για ένα μικρό `max_recommendations` (2 ή 3). Αν σε κάποιες ταινίες το σύστημα μοιάζει να επιστρέφει σημασιολογικά κοντινές ταινίες σημειώστε το `ID` τους. Δοκιμάστε στη συνέχεια να βελτιστοποιήσετε την `TfidfVectorizer` για τα συγκεκριμένα `ID` ώστε να επιστρέφονται σημασιολογικά κοντινές ταινίες για μεγαλύτερο αριθμό `max_recommendations`. Παράλληλα, όσο βελτιστοποιείτε την `TfidfVectorizer`, θα πρέπει να λαμβάνετε καλές συστάσεις για μεγαλύτερο αριθμό τυχαίων ταινιών. Μπορείτε επίσης να βελτιστοποιήσετε τη συνάρτηση παρατηρώντας πολλά φαινόμενα που το σύστημα εκλαμβάνει ως ομοιότητα περιεχομένου ενώ επί της ουσίας δεν είναι επιθυμητό να συνυπολογίζονται (δείτε σχετικά το [FAQ](https://docs.google.com/document/d/1hou1gWXQuHAB7J2aV44xm_CtAWJ63q6Cu1V6OwyL_n0/edit?usp=sharing)). Ταυτόχρονα, μια άλλη κατεύθυνση της βελτιστοποίησης είναι να χρησιμοποιείτε τις παραμέτρους του `TfidfVectorizer` έτσι ώστε να μειώνονται οι διαστάσεις του Vector Space Model μέχρι το σημείο που θα αρχίσει να εμφανίζονται επιπτώσεις στην ποιότητα των συστάσεων.
"""

content_recommender(10, 2)

"""Παρατηρούμε ότι τα αποτελεσμάτα δεν είναι βελτιστοποιημένα, αλλά δεν είναι και τελείως άσχετα μεταξύ τους, καθώς υπάρχει για παράδειγμα η κατηγορία Action στην 2η πρόταση. """

content_recommender(17, 2)

"""Εδώ παρατηρούμε ότι οι 2 προτάσεις μεταξύ τους είναι σχετικά κοινές. Η συσχέτιση τους όμως με την κύρια ταινία που τέθηκε προς αναζήτηση συστάσεων είναι αμφισβητούμενη."""

content_recommender(42, 3)

"""Εδώ βλέπουμε ότι η κατηγορία Drama αποτελεί κοινή συνιστώσα μεταξύ των ταινιών."""

#αρχική διάσταση του corpus μας / από πόσες λέξεις αποτελείται

import nltk
nltk.download('punkt') # χρειάζεται για το tokenizer
words = []
for doc in corpus:
    words += nltk.word_tokenize(doc)
print("Total words in corpus:\t", len(words))

"""Αρχικά υπολογίζουμε τον συνολικό αριθμό λέξεων που υπάρχει στο dataset μας. Αυτός ανέρχεται στις 1775929 λέξεις. Στόχος μας θα είναι η μείωση αυτών αφαιρώντας λέξεις, οι οποίες δεν μας προσφέρουν σημαντικά κριτήρια για την ομοιότητα των ταινιών. Έτσι, θα πρέπει να αφαιρέσουμε τέτοιες λέξεις από το dataset μας, ώστε να οδηγηθούμε σε ασφαλέστερα συμπεράσματα. """

nltk.download('stopwords') # κατεβάζουμε ένα αρχείο που έχει stopwords στα αγγλικά
from nltk.corpus import stopwords
import string

filtered_words = [word for word in words if word not in stopwords.words('english') + list(string.punctuation)]

print("Total words in corpus:\t", len(filtered_words))

"""Σαν αρχική επεξεργασία των λεγόμενων stop words, εισάγουμε τις βασικές stopwords των αγγλικών και τα σημεία στίξης. Ήδη, οι συνολικές λέξεις του dataset μας μειώθηκε στις 927061 λέξεις.

Θέλουμε να προσθέσουμε και άλλες stopwords που προκύπτουν από τις παρατηρήσεις μας πάνω στο dataset.
"""

#https://www.matthewjockers.net/macroanalysisbook/expanded-stopwords-list/
new_stop_words = pd.read_csv('./mystopwords', sep=',', header=None, error_bad_lines=False, skipinitialspace=True)
for i in new_stop_words.values:
  new_stop_words = list(i) 
new_stop_words

"""Κατά την παρατήρηση των κειμένων περιγραφής παρατηρήσαμε ότι τα ονόματα χαρακτήρων δημιουργούσαν πολύ συχνά σύγχυση στις ταινίες που πρότεινε το σύστημα. Γι' αυτό προσθέσαμε το αρχείο "mystopwords", που περιέχει ένα [κατάλογο](https://www.matthewjockers.net/macroanalysisbook/expanded-stopwords-list/) με stop words που περιλαμβάνει κύρια ονόματα."""

new_stop_words= new_stop_words+["story","film","movie", "plot","about", "someone", "starts", "meet", "want", "told", "'s"]

"""Στη συνέχεια, όπως φαίνεται παραπάνω επεκτείναμε το αρχείο με τις stop words μας κάποιες βασικές λέξεις, οι οποίες θα μπορούσαν να δημιουργήσουν πρόβλημα στο συγκεκριμένο dataset. Έτσι, αφού πρόκειται για ένα σύνολο ταινιών, λέξεις όπως film, movie, plot, story είναι πολύ κοινές και για αυτό τις αφαιρούμε. Επιπλέον, συναντήσαμε άλλες πολύ κοινές λέξεις που σχετίζονται με την προσπάθεια περιγραφής του περιεχομένου μιας ταινίας, καθώς φράσεις όπως "the film is about someone...." ή "the movie starts with ...." είναι πολύ συνηθισμένες. Έτσι, προσπαθήσαμε να συμπεριλάβουμε και τέτοιες παρόμοιες stop words. """

final_words = [word for word in filtered_words if word not in new_stop_words]

print("Total words in corpus:\t", len(final_words))

"""Τελικώς, παρατηρούμε ότι οι λέξεις μας μειώθηκαν σε 816065. """

nltk.download('wordnet') # απαραίτητα download για τους stemmer/lemmatizer
nltk.download('rslp')

"""Στη συνέχεια δημιουργούμε δύο συναρτήσεις που περιλαμβάνουν όλη την παρπάνω προεπεξεργασία."""

import nltk
nltk.download('punkt') 
nltk.download('stopwords')
from nltk.corpus import stopwords
import string
import collections
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer

wordnet_lemmatizer = WordNetLemmatizer()
porter_stemmer = PorterStemmer()

new_stop_words = pd.read_csv('./mystopwords', sep=',', header=None, error_bad_lines=False, skipinitialspace=True)
for i in new_stop_words.values:
  new_stop_words = list(i) 
new_stop_words= new_stop_words+["story","film","movie", "plot","about", "someone", "starts", "meet", "want", "told", "'s", "poe"]

def thorough_filter(words):
    filtered_words = []
    for word in words:
        pun = []
        for letter in word:
            pun.append(letter in string.punctuation)
        if not all(pun):
            filtered_words.append(word)
    return filtered_words

def preprocess_document(document):
    # όλα τα προηγούμενα βήματα που κάναμε μέχρι στιγμής
    words = nltk.word_tokenize(document.lower())
    filtered_words = [word for word in words if word not in stopwords.words('english') + list(string.punctuation) + new_stop_words]
    filtered_words = thorough_filter(filtered_words)
    stemmed_words = [porter_stemmer.stem(word) for word in filtered_words]
    return (" ".join(stemmed_words))

"""Στην παραπάνω προεπεξεργασία, χρησιμοποιούμε τις stopwords όπως ακριβώς και παραπάνω, προσθέτοντας ένα φίλρο **stemmer**. Συγκεκριμένα η λειτουργία αυτού του φίλτρου είναι να απομακρύνει τα άκρα των λέξεων με την ελπίδα να φέρει τις λέξεις σε μια κοινή μορφή βάσης (ουσιαστικά πρόκειται για αφαίρεση καταλήξεων). Οι τεχνικές που χρησιμοποιούνται είναι 2 : stemming και lemmatization. Δεν μπορούμε να χρησιμοποιήσουμε και τις 2 ταυτόχρονα. Η επιλογή της χρήσης stemming έγινε με δεδομένο ότι πρόκειται για την πιο συχνή. """

final_corpus = [preprocess_document(doc) for doc in corpus]
print(final_corpus[0])

vectorizer = TfidfVectorizer()
vectorizer.fit(final_corpus)
corpus_tf_idf = vectorizer.transform(final_corpus)

print(corpus_tf_idf.shape) #μειώθηκε

"""Παρατηρούμε ότι και οι συνολικές διαστάσεις του corpus μας μειώθηκαν. """

content_recommender(10, 10)

"""Παρατηρούμε ήδη εντυπωσιακές αλλαγές από το απλό σύστημα προτάσεων που είχαμε πριν. Συγκεκριμένα, οι δύο προτάσεις που έγιναν πριν για την συγκεκριμένη αυτή ταινία, τώρα δεν υπάρχουν καν στις 10 πρώτες συστάσεις. Αντιθέτως, προτείνονται ταινίες που φαίνεται να έχουν αρκετή περισσότερη συσχέτιση με την ταινία αναφοράς. Απόδειξη αυτού αποτελεί η πολύ συχνή εμφάνιση κοινών κατηγοριών, όπως Action/Adventure και Western. 

Επομένως, ήδη έχουμε πετύχει μια πολύ μεγάλη βελτίωση στο σύστημα μας.

Συνεχίζουμε με βελτιστοποίηση των παραμέτρων της συνάρτησης.
"""

vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 10, max_features=5000, use_idf=False, sublinear_tf=True)
vectorizer.fit(final_corpus)
corpus_tf_idf = vectorizer.transform(final_corpus)

"""Αναλύουμε την επιλογή της κάθε παραμέτρου ξεχωριστά : 

**max_df** : η παραμέτρος αυτή αφορά την αφαίρεση λέξεων που χρησιμοποιούνται αρκετά συχνά μέσα στις περιγραφές των διάφορων ταινιών. Ουσιαστικά, πρόκειται για μια μεταβλητή, ενδεικτική της εμφάνισης κάθε λέξης στο σύνολο του dataset μας. Παρατηρήσαμε, ότι η μεταβολή της συγκεκριμένης παραμέτρου δεν μεταβάλλει πραγματικά το shape του corpus, καθώς οι πολύ συχνές λέξεις αφαιρέθηκαν ήδη κατά την δημιουργία των stop words. Πειραματικά, ρυθμίσαμε την τιμή ίση με 0.5 (κόβει λέξεις που εμφανίζονται στις μισές περιγραφές).

**min_df** : πρόκειται για μεταβλητή που αφαιρεί πολύ σπάνιες λέξεις. Επιλέξαμε πειραματικά τον αριθμό 10, ώστε να κόβονται οι λέξεις που εμφανίζονται σε λιγότερο από 10 ταινίες. 

**max_features** : Με βάση την παραπάνω επεξεργασία είχαμε ένα corpus 5000x5453. Έτσι επιλέγουμε να κρατήσουμε τις 5000 πιο συχνές λέξεις ρυθμίζοντας το με αυτή την μεταβλητή. 

**use_idf** : Η εν λόγω εφαρμογή δε λειτουργεί καλά εάν αυτή η παράμετρος ειναι αληθής. Η παράμετρος σχετίζεται με μεταβολή των βαρών κάθε token-λέξης αντίστροφα προς τη συχνότητα της λέξης σε όλα τα documents. Ωστόσο, εδώ η συχνότητα των σημαντικών λέξεων στο document καθορίζει το θέμα του. Επομένως είναι σημαντικό τα βάρη να είναι σύμφωνα με αυτή. Αυτό συμβαίνει γιατί φροντίσαμε από την προεπεξεργασία να αφαιρέσουμε λέξεις που θα ήταν πολύ συχνές (π.χ. άρθρα) αλλά χωρίς να παίζουν ρόλο στη σημασιολογία του εγγράφου.

**sublinear_tf** : Το scaling θα είναι σύμφωνα με το 1+log(tf).  

"""

print(corpus_tf_idf.shape) #μειώθηκε

"""###10 ΠΑΡΑΔΕΙΓΜΑΤΑ ΤΟΥ ΣΥΣΤΗΜΑΤΟΣ"""

content_recommender(10, 10)

"""Παρατηρούμε αρκετές μεταβολές στα αποτελεσμάτα πλέον και από τα ενδιάμεσα αποτελέσματα που είχαμε. Πλέον, μπορούμε να παρατηρήσουμε ότι και οι 10 συστάσεις ανήκουν στην ίδια κατηγορία με την ταινία αναφοράς, δηλαδή πρόκειται για ταινίες Action/Adventure και Western με αρκετά κοινό περιεχόμενο."""

content_recommender(3, 10)

"""Βλέπουμε ότι και εδώ υπάρχουν αρκετά κοινά μεταξύ της ταινίας αναφοράς και των συστάσεων και επομένως, μπορεί να κριθεί επιτυχές το σύστημα μας. Συγκεκριμένα, έχουμε πολλές κοινές κατηγορίες όπως το Drama και το Action, Crime Fiction, Thriller και υπάρχει κοινή θεματική γύρω από την φυλακή."""

content_recommender(100, 10)

"""Και εδώ βλέπουμε ότι το Science Fiction που αποτελεί μοναδική κατηγορία της ταινίας αναφοράς μας, εμφανίζεται στην πλειοψηφία των συστάσεων. Μάλιστα οι ταινίες που προτείνονται αφορούν εξωγήινους. Αυτό σημαίνει ότι το σύστημά μας πέτυχε να αναγνωρίσει τις λέξεις εκείνες που περιγράφουν το θέμα της ταινίας και να τις εντοπίσει σε εκείνες που προτείνει."""

content_recommender(50, 10)

"""Εδώ κοινή θεματική αποτελεί το crime fiction και το Drama. Επιπλέον, η αρχική μας ταινία σχετίζεται με μουσική και χορό πράγμα που το σύστημα καταφέρνει να εντοπίσει και να προτείνει παρόμοιες ταινίες. Παρατηρούμε, επίσης, ότι προτείνονται ταινίες με θεματική την οικογένεια. Η αρχική μας ταινία αναφέρεται συχνά στη λέξη "brother" πράγμα που προφανώς ευνοεί αυτές τις προτάσεις."""

content_recommender(500, 10)

"""Στο συγκεκριμένο παράδειγμα μας, η ταινία αναφοράς τυχαίνει να αφορά πολλές διαφορετικές κατηγορίες. Παρατηρούμε ότι και με τις 10 πρώτες συστάσεις, υπάρχει τουλάχιστον 1 κοινή κατηγορία (εκτός από την 6 και 7, οι οποίες όμως έχουν κοινή κατηγορία μεταξύ τους). Παρατηρούμε μια κοινή θεματική με όλες τις ταινίες με κύρια στοιχεία την ύπαρξη detective στην πλοκή και δολοφονιών."""

content_recommender(2500, 10)

"""Κοινή θεματική των παραπάνω ταινιών είναι το Drama και το crime fiction. Υπάρχουν και κάποια περίεργα αποτελέσματα, ωστόσο, όπως το 10ο που πρόκειται για μια animation ταινία."""

content_recommender(422, 10)

"""Το συγκεκριμένο παράδειγμα είναι αρκετά μπερδευτικό, καθώς η ταινία αναφοράς είναι μια short-film που ξεκινάει με δραματικά στοιχεία και καταλήγει ως κωμωδία, καθώς μέσα σε μια δραματική κατάσταση υπάρχει μια κληρονομιά. Αυτά τα στοιχεία μεταδίδονται και στις συστάσεις, καθώς προτείνονται κυρίως δράματα, αλλά και ταινίες με θεματικές την κληρονομιά."""

content_recommender(3741, 10)

"""Άλλο ένα παράδειγμα που η κύρια κατηγορία είναι το Drama και αυτό καταφέρνει να μεταφέρεται σε όλες τις συστάσεις."""

content_recommender(4500, 10)

"""Η παραπάνω ταινία ανήκει στην κατηγορία της βιογραφίας-ντοκιμαντερ. Γνωρίζοντας πώς λειτουργεί το σύστημά μας καταλαβαίνουμε ότι πρόκειται για μια δύσκολη κατηγορία. Αυτό επειδή δεν υπάρχουν συγκεκριμένα στοιχεία στην περιγραφή της ταινίας που να δηλώνουν το είδος της, είναι μια ιστορία όπως η περιγραφή οποιασδήποτε ταινίας. Ωστόσο, στα αποτελέσματα έχουμε δύο προτάσεις που ανήκουν στην ίδια κατηγορία. Πράγματι, πρόκειται για βιογραφικές ταινίες με την ίδια θεματολογία (γύρω από τη μουσική). Ωστόσο, θα υπήρχαν πολλές ταινίες με μουσική στη βάση μας. Το γεγονός ότι επιλέχθηκαν και κάποιες που αποτελούν πράγματι βιογραφίες-ντοκιμαντερ είναι ιδιαίτερα ενθαρρυντικό.  """

content_recommender(3685, 10)

"""Πρόκειται για ομοίως δύσκολη κατηγορία για να γίνει αντιληπτή από την περιγραφή της ταινίας. Βλέπουμε ότι εδώ το σύστημα δυσκολεύται περισσότερο. Μόνο η ένατη πρόταση είναι animation. Ωστόσο, οι προτάσεις του συστήματος ως προς τη θεματολογία είναι πολύ καλές. Πρόκειται για μια ταινία με δράκους και δράση. Παρατηρούμε ότι πολλές από τις προτάσεις περιλαμβάνουν περιπέτεια, φαντασία και μαγεία που συνάδουν με αυτό το θέμα. Ωστόσο, το σύστημα φαίνεται να βρίσκει μεγάλη ομοιότητα και με συστήματα που περιλαμβάνου στις περιγραφές τους links. Το γεγονός αυτό μας δείχνει ότι υπάρχουν μεγάλα περιθώρια βελτίωσης.

## Επεξήγηση επιλογών και ποιοτική ερμηνεία

Σε markdown περιγράψτε πώς προχωρήσατε στις επιλογές σας για τη βελτιστοποίηση της `TfidfVectorizer`. Επίσης σε markdown δώστε 10 παραδείγματα (IDs) από τη συλλογή σας που επιστρέφουν καλά αποτελέσματα μέχρι `max_recommendations` (5 και παραπάνω) και σημειώστε συνοπτικά ποια είναι η θεματική που ενώνει τις ταινίες.

Δείτε [εδώ](https://pastebin.com/raw/ZEvg5t3z) ένα παράδειγμα εξόδου του βελτιστοποιημένου συστήματος συστάσεων για την ταίνία ["Q Planes"](https://en.wikipedia.org/wiki/Q_Planes) με την κλήση της συνάρτησης για κάποιο seed `content_recommender(529,3)`. Είναι φανερό ότι η κοινή θεματική των ταινιών είναι τα αεροπλάνα, οι πτήσεις, οι πιλότοι, ο πόλεμος.

## Tip: persistence αντικειμένων με joblib.dump

H βιβλιοθήκη [joblib](https://pypi.python.org/pypi/joblib) της Python δίνει κάποιες εξαιρετικά χρήσιμες ιδιότητες στην ανάπτυξη κώδικα: pipelining, παραλληλισμό, caching και variable persistence. Τις τρεις πρώτες ιδιότητες τις είδαμε στην πρώτη άσκηση. Στην παρούσα άσκηση θα μας φανεί χρήσιμη η τέταρτη, το persistence των αντικειμένων. Συγκεκριμένα μπορούμε με:

```python
joblib.dump(my_object, 'my_object.pkl') 
```

να αποθηκεύσουμε οποιοδήποτε αντικείμενο-μεταβλητή (εδώ το `my_object`) απευθείας πάνω στο filesystem ως αρχείο, το οποίο στη συνέχεια μπορούμε να ανακαλέσουμε ως εξής:

```python
my_object = joblib.load('my_object.pkl')
```

Μπορούμε έτσι να ανακαλέσουμε μεταβλητές ακόμα και αφού κλείσουμε και ξανανοίξουμε το notebook, χωρίς να χρειαστεί να ακολουθήσουμε ξανά όλα τα βήματα ένα - ένα για την παραγωγή τους, κάτι ιδιαίτερα χρήσιμο αν αυτή η διαδικασία είναι χρονοβόρα.

Ας αποθηκεύσουμε το `corpus_tf_idf` και στη συνέχεια ας το ανακαλέσουμε.
"""

import joblib

#joblib.dump(final_corpus, 'final_corpus.pkl') 
joblib.dump(corpus_tf_idf, 'corpus_tf_idf.pkl') 
#joblib.dump(som, 'som.pkl')

"""

Μπορείτε με ένα απλό `!ls` να δείτε ότι το αρχείο `corpus_tf_idf.pkl` υπάρχει στο filesystem σας (== persistence):"""

!ls -lh

from google.colab import drive
drive.mount('/content/drive')

"""και μπορούμε να τα διαβάσουμε με `joblib.load`"""

final_corpus = joblib.load('final_corpus.pkl')
corpus_tf_idf = joblib.load('corpus_tf_idf.pkl')
som = joblib.load('som.pkl')

"""# Εφαρμογή 2.  Τοπολογική και σημασιολογική απεικόνιση της ταινιών με χρήση SOM
<img src="https://i.imgur.com/Z4FdurD.jpg" width="60%">

## Δημιουργία dataset
Στη δεύτερη εφαρμογή θα βασιστούμε στις τοπολογικές ιδιότητες των Self Organizing Maps (SOM) για να φτιάξουμε ενά χάρτη (grid) δύο διαστάσεων όπου θα απεικονίζονται όλες οι ταινίες της συλλογής της ομάδας με τρόπο χωρικά συνεκτικό ως προς το περιεχόμενο και κυρίως το είδος τους (ο παραπάνω χάρτης είναι ενδεικτικός, δεν αντιστοιχεί στο dataset μας). 

Η `build_final_set` αρχικά μετατρέπει την αραιή αναπαράσταση tf-idf της εξόδου της `TfidfVectorizer()` σε πυκνή (η [αραιή αναπαράσταση](https://en.wikipedia.org/wiki/Sparse_matrix) έχει τιμές μόνο για τα μη μηδενικά στοιχεία). 

Στη συνέχεια ενώνει την πυκνή `dense_tf_idf` αναπαράσταση και τις binarized κατηγορίες `catbins` των ταινιών ως επιπλέον στήλες (χαρακτηριστικά). Συνεπώς, κάθε ταινία αναπαρίσταται στο Vector Space Model από τα χαρακτηριστικά του TFIDF και τις κατηγορίες της.

Τέλος, δέχεται ένα ορισμα για το πόσες ταινίες να επιστρέψει, με default τιμή όλες τις ταινίες (5000). Αυτό είναι χρήσιμο για να μπορείτε αν θέλετε να φτιάχνετε μικρότερα σύνολα δεδομένων ώστε να εκπαιδεύεται ταχύτερα το SOM.
"""

def build_final_set(doc_limit = 5000, tf_idf_only=False):
    # convert sparse tf_idf to dense tf_idf representation
    dense_tf_idf = corpus_tf_idf.toarray()[0:doc_limit,:]
    if tf_idf_only:
        # use only tf_idf
        final_set = dense_tf_idf
    else:
        # append the binary categories features horizontaly to the (dense) tf_idf features
        final_set = np.hstack((dense_tf_idf, catbins[0:doc_limit,:]))
        # η somoclu θέλει δεδομένα σε float32
    return np.array(final_set, dtype=np.float32)

final_set = build_final_set()

"""Τυπώνουμε τις διαστάσεις του τελικού dataset μας. Χωρίς βελτιστοποίηση του TFIDF θα έχουμε περίπου 50.000 χαρακτηριστικά."""

final_set.shape

"""Με βάση την εμπειρία σας στην προετοιμασία των δεδομένων στην επιβλεπόμενη μάθηση, υπάρχει κάποιο βήμα προεπεξεργασίας που θα μπορούσε να εφαρμοστεί σε αυτό το dataset;

Προκειμένου να μειώσουμε επιπλέον την διάσταση του dataset μας θα μπορούσαμε να εφαρμόσουμε ξανά την παραπάνω συνάρτηση TfidfVectorizer με αυστηρότερα, όμως, κριτήρια για τις παραμέτρους max_df και min_df, ώστε να μείνουν και λιγότερα δεδομένα.

Ένας άλλος τρόπος που μπορεί να εφαρμοστεί κατά το βήμα της προεπεξεργασίας των δεδομένων είναι η τεχνική PCA.

## **PCA**
"""

from sklearn.decomposition import PCA

pca = PCA(n_components=0.98)
final_set = pca.fit_transform(final_set)

final_set.shape

"""Παρατηρούμε ότι μπορούμε να διατηρήσουμε το 98% της πληροφορίας και να μειώσουμε την διάσταση του dataset μας υπερβολικά από 5322 σε 1979 "features".

## Εκπαίδευση χάρτη SOM

Θα δουλέψουμε με τη βιβλιοθήκη SOM ["Somoclu"](http://somoclu.readthedocs.io/en/stable/index.html). Εισάγουμε τις somoclu και matplotlib και λέμε στη matplotlib να τυπώνει εντός του notebook (κι όχι σε pop up window).
"""

# Commented out IPython magic to ensure Python compatibility.
# install somoclu
!pip install --upgrade somoclu
# import sompoclu, matplotlib
import somoclu
import matplotlib
# we will plot inside the notebook and not in separate window
# %matplotlib inline

"""Καταρχάς διαβάστε το [function reference](http://somoclu.readthedocs.io/en/stable/reference.html) του somoclu. Θα δoυλέψουμε με χάρτη τύπου planar, παραλληλόγραμμου σχήματος νευρώνων με τυχαία αρχικοποίηση (όλα αυτά είναι default). Μπορείτε να δοκιμάσετε διάφορα μεγέθη χάρτη ωστόσο όσο ο αριθμός των νευρώνων μεγαλώνει, μεγαλώνει και ο χρόνος εκπαίδευσης. Για το training δεν χρειάζεται να ξεπεράσετε τα 100 epochs. Σε γενικές γραμμές μπορούμε να βασιστούμε στις default παραμέτρους μέχρι να έχουμε τη δυνατότητα να οπτικοποιήσουμε και να αναλύσουμε ποιοτικά τα αποτελέσματα. Ξεκινήστε με ένα χάρτη 10 x 10, 100 epochs training και ένα υποσύνολο των ταινιών (π.χ. 2000). Χρησιμοποιήστε την `time` για να έχετε μια εικόνα των χρόνων εκπαίδευσης. Ενδεικτικά, με σωστή κωδικοποίηση tf-idf, μικροί χάρτες για λίγα δεδομένα (1000-2000) παίρνουν γύρω στο ένα λεπτό ενώ μεγαλύτεροι χάρτες με όλα τα δεδομένα μπορούν να πάρουν 10-15 λεπτά ή και περισσότερο.

Θα δουλέψουμε με 3 διαφορετικές περιπτώσεις χαρτών. Θα εκπαιδεύσουμε 3 διαφορετικά μοντέλα, διαστάσεων **10x10**, **20x20** και **30x30**. Παρακάτω παρουσιάζεται όλη η διαδικασία εκπαίδευσης των 3 περιπτώσεων. Αποθηκεύουμε τα 3 som σε .pkl, καθώς έτσι γλιτώνουμε σημαντικό χρόνο.
Και στις 3 περιπτώσεις, εκπαιδεύουμε σε 100 εποχές.

### **10x10**
"""

# Commented out IPython magic to ensure Python compatibility.
#data = build_final_set(2000)

n_rows, n_columns = 10, 10
som10 = somoclu.Somoclu(n_columns, n_rows)
# %time som10.train(final_set, epochs=100)

"""### **20x20**"""

# Commented out IPython magic to ensure Python compatibility.
n_rows, n_columns = 20, 20
som20 = somoclu.Somoclu(n_columns, n_rows)
# %time som20.train(final_set, epochs=100)

"""### **25x25**"""

# Commented out IPython magic to ensure Python compatibility.
n_rows, n_columns = 25, 25
som25 = somoclu.Somoclu(n_columns, n_rows)
# %time som25.train(final_set, epochs=100)

"""### **30x30**"""

# Commented out IPython magic to ensure Python compatibility.
n_rows, n_columns = 30, 30
som30 = somoclu.Somoclu(n_columns, n_rows)
# %time som30.train(final_set, epochs=100)

import joblib

som10 = joblib.load('som10.pkl') 
som20 = joblib.load('som20.pkl') 
som25 = joblib.load('som25.pkl') 
som30 = joblib.load('som30.pkl')

"""## Best matching units

Μετά από κάθε εκπαίδευση αποθηκεύστε σε μια μεταβλητή τα best matching units (bmus) για κάθε ταινία. Τα bmus μας δείχνουν σε ποιο νευρώνα ανήκει η κάθε ταινία. Προσοχή: η σύμβαση των συντεταγμένων των νευρώνων είναι (στήλη, γραμμή) δηλαδή το ανάποδο από την Python. Με χρήση της [np.unique](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html) (μια πολύ χρήσιμη συνάρτηση στην άσκηση) αποθηκεύστε τα μοναδικά best matching units και τους δείκτες τους (indices) προς τις ταινίες. Σημειώστε ότι μπορεί να έχετε λιγότερα μοναδικά bmus από αριθμό νευρώνων γιατί μπορεί σε κάποιους νευρώνες να μην έχουν ανατεθεί ταινίες. Ως αριθμό νευρώνα θα θεωρήσουμε τον αριθμό γραμμής στον πίνακα μοναδικών bmus.

### **10x10**
"""

bmus10 = som10.bmus
print(bmus10.shape)

ubmus10, indices10 = np.unique(bmus10, return_inverse=True, axis=0)

print(ubmus10.shape)
print(indices10.shape)

"""### **20x20**"""

bmus20 = som20.bmus
print(bmus20.shape)

ubmus20, indices20 = np.unique(bmus20, return_inverse=True, axis=0)

print(ubmus20.shape)
print(indices20.shape)

"""### **25x25**"""

bmus25 = som25.bmus
print(bmus25.shape)

ubmus25, indices25 = np.unique(bmus25, return_inverse=True, axis=0)

print(ubmus25.shape)
print(indices25.shape)

"""### **30x30**"""

bmus30 = som30.bmus
print(bmus30.shape)

ubmus30, indices30 = np.unique(bmus30, return_inverse=True, axis=0)

print(ubmus30.shape)
print(indices30.shape)

"""Παρατηρούμε ότι για χάρτες 20x20, 25x25 και 30x30 έχουμε νευρώνες που δε νίκησαν ποτέ. Έτσι, το μέγεθος των μοναδικών νευρώνων είναι μικρότερο από 400 (358), 625 (549) και 900 (755) αντίστοιχα.

## Ομαδοποίηση (clustering)

Τυπικά, η ομαδοποίηση σε ένα χάρτη SOM προκύπτει από το unified distance matrix (U-matrix): για κάθε κόμβο υπολογίζεται η μέση απόστασή του από τους γειτονικούς κόμβους. Εάν χρησιμοποιηθεί μπλε χρώμα στις περιοχές του χάρτη όπου η τιμή αυτή είναι χαμηλή (μικρή απόσταση) και κόκκινο εκεί που η τιμή είναι υψηλή (μεγάλη απόσταση), τότε μπορούμε να πούμε ότι οι μπλε περιοχές αποτελούν clusters και οι κόκκινες αποτελούν σύνορα μεταξύ clusters.

To somoclu δίνει την επιπρόσθετη δυνατότητα να κάνουμε ομαδοποίηση των νευρώνων χρησιμοποιώντας οποιονδήποτε αλγόριθμο ομαδοποίησης του scikit-learn. Στην άσκηση θα χρησιμοποιήσουμε τον k-Means. Για τον αρχικό σας χάρτη δοκιμάστε ένα k=20 ή 25. Οι δύο προσεγγίσεις ομαδοποίησης είναι διαφορετικές, οπότε περιμένουμε τα αποτελέσματα να είναι κοντά αλλά όχι τα ίδια.

### **10x10**

#### 20 clusters
"""

from sklearn.cluster import KMeans
algorithm = KMeans(n_clusters = 20)
som10.cluster(algorithm=algorithm)

"""#### 25 clusters"""

from sklearn.cluster import KMeans
algorithm = KMeans(n_clusters = 25)
som10.cluster(algorithm=algorithm)

"""### **20x20**

#### 20 clusters
"""

from sklearn.cluster import KMeans
algorithm = KMeans(n_clusters = 20)
som20.cluster(algorithm=algorithm)

"""#### 25 clusters"""

from sklearn.cluster import KMeans
algorithm = KMeans(n_clusters = 25)
som20.cluster(algorithm=algorithm)

"""### **25x25**

#### 20 clusters
"""

from sklearn.cluster import KMeans
algorithm = KMeans(n_clusters = 20)
som25.cluster(algorithm=algorithm)

"""#### 25 clusters"""

from sklearn.cluster import KMeans
algorithm = KMeans(n_clusters = 25)
som25.cluster(algorithm=algorithm)

"""### **30x30**

#### 25 clusters
"""

from sklearn.cluster import KMeans
algorithm = KMeans(n_clusters = 25)
som30.cluster(algorithm=algorithm)

"""#### 30 clusters"""

from sklearn.cluster import KMeans
algorithm = KMeans(n_clusters = 30)
som30.cluster(algorithm=algorithm)

"""
## Αποθήκευση του SOM

Επειδή η αρχικοποίηση του SOM γίνεται τυχαία και το clustering είναι και αυτό στοχαστική διαδικασία, οι θέσεις και οι ετικέτες των νευρώνων και των clusters θα είναι διαφορετικές κάθε φορά που τρέχετε τον χάρτη, ακόμα και με τις ίδιες παραμέτρους. Για να αποθηκεύσετε ένα συγκεκριμένο som και clustering χρησιμοποιήστε και πάλι την `joblib`. Μετά την ανάκληση ενός SOM θυμηθείτε να ακολουθήσετε τη διαδικασία για τα bmus.
"""

import joblib
 
joblib.dump(som10, 'som10.pkl') 
joblib.dump(som20, 'som20.pkl') 
joblib.dump(som25, 'som25.pkl') 
joblib.dump(som30, 'som30.pkl')

"""## Οπτικοποίηση U-matrix, clustering και μέγεθος clusters

Για την εκτύπωση του U-matrix χρησιμοποιήστε τη `view_umatrix` με ορίσματα `bestmatches=True` και `figsize=(15, 15)` ή `figsize=(20, 20)`. Τα διαφορετικά χρώματα που εμφανίζονται στους κόμβους αντιπροσωπεύουν τα διαφορετικά clusters που προκύπτουν από τον k-Means. Μπορείτε να εμφανίσετε τη λεζάντα του U-matrix με το όρισμα `colorbar`. Μην τυπώνετε τις ετικέτες (labels) των δειγμάτων, είναι πολύ μεγάλος ο αριθμός τους.

Για μια δεύτερη πιο ξεκάθαρη οπτικοποίηση του clustering τυπώστε απευθείας τη μεταβλητή `clusters`.

Τέλος, χρησιμοποιώντας πάλι την `np.unique` (με διαφορετικό όρισμα) και την `np.argsort` (υπάρχουν και άλλοι τρόποι υλοποίησης) εκτυπώστε τις ετικέτες των clusters (αριθμοί από 0 έως k-1) και τον αριθμό των νευρώνων σε κάθε cluster, με φθίνουσα ή αύξουσα σειρά ως προς τον αριθμό των νευρώνων. Ουσιαστικά είναι ένα εργαλείο για να βρίσκετε εύκολα τα μεγάλα και μικρά clusters. 

Ακολουθεί ένα μη βελτιστοποιημένο παράδειγμα για τις τρεις προηγούμενες εξόδους:

<img src="https://image.ibb.co/i0tsfR/umatrix_s.jpg" width="35%">
<img src="https://image.ibb.co/nLgHEm/clusters.png" width="35%">

### **10x10**

#### 20 clusters
"""

som10.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som10.clusters)

clusters10_20, neurons_in_clusters10_20 = np.unique(som10.clusters, return_inverse=True)

print(clusters10_20.shape)
print(neurons_in_clusters10_20.shape)

appearences10_20 = np.bincount(neurons_in_clusters10_20)
sorted10_20 = np.argsort(appearences10_20)
print("Clusters sorted by increasing number of neurons:")
print("Cluster index  | Number of neurons ")
print("---------------------------------------------------")
for i in range(len(clusters10_20)):
  if (clusters10_20[sorted10_20[19-i]]>9):
    print(" {}            | {}".format(clusters10_20[sorted10_20[19-i]], appearences10_20[sorted10_20[19-i]]))
  else :
    print(" {}             | {}".format(clusters10_20[sorted10_20[19-i]], appearences10_20[sorted10_20[19-i]]))

"""#### 25 clusters"""

som10.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som10.clusters)

clusters10_25, neurons_in_clusters10_25 = np.unique(som10.clusters, return_inverse=True)

print(clusters10_25.shape)
print(neurons_in_clusters10_25.shape)

appearences10_25 = np.bincount(neurons_in_clusters10_25)
sorted10_25 = np.argsort(appearences10_25)
print("Clusters sorted by increasing number of neurons:")
print("Cluster index  | Number of neurons ")
print("---------------------------------------------------")
for i in range(len(clusters10_25)):
  if (clusters10_25[sorted10_25[24-i]]>9):
    print(" {}            | {}".format(clusters10_25[sorted10_25[24-i]], appearences10_25[sorted10_25[24-i]]))
  else :
    print(" {}             | {}".format(clusters10_25[sorted10_25[24-i]], appearences10_25[sorted10_25[24-i]]))

"""Κατ' αρχάς, παρατηρούμε γενικά ότι ο k-Means σέβεται τα όρια των περιοχών του χάρτη SOM. Αυτό συμβαίνει περισσότερο για τις μπλε περιοχές, ενώ για τις κόκκινες είναι πιο έντονη και συχνή η διαφορετική ομαδοποιήση των στοιχείων. Το γεγονός αυτό είναι λογικό, καθώς οι κόκκινες περιοχές είναι συνοριακές. Άρα, οι ταινίες που ανήκουν σε αυτές είναι πιθανό να δημιουργούν σύγχυση ως προς το είδος τους, γι'αυτό και διαχωρίζονται συχνότερα από τον k-Means.  

Σημειώνουμε μία αναμενόμενη διαφορά: με την αύξηση του αριθμού των clusters μειώθηκαν οι νευρώνες που ανήκουν στο ίδιο cluster. Θα μπορούσαμε να παρομοιάσουμε την αύξηση των clusters ως εντοπισμό υποκατηγοριών. Δηλαδή, ο πρώτος χάρτης είχε σε κοινό cluster τα πάνω αριστερά στοιχεία, κατατάσσοντάς τα στην ίδια κατηγορία ταινιών. Ο δεύτερος χάρτης τα διαχώρισε, εντοπίζοντας κάποια διαφορά μεταξύ τους, άρα βρίσκοντας κάποιες υποκατηγορίες.

### **20x20**

#### 20 clusters
"""

som20.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som20.clusters)

clusters20_20, neurons_in_clusters20_20 = np.unique(som20.clusters, return_inverse=True)

print(clusters20_20.shape)
print(neurons_in_clusters20_20.shape)

appearences20_20 = np.bincount(neurons_in_clusters20_20)
sorted20_20 = np.argsort(appearences20_20)
print("Clusters sorted by increasing number of neurons:")
print("Cluster index  | Number of neurons ")
print("---------------------------------------------------")
for i in range(len(clusters20_20)):
  if (clusters20_20[sorted20_20[19-i]]>9):
    print(" {}            | {}".format(clusters20_20[sorted20_20[19-i]], appearences20_20[sorted20_20[19-i]]))
  else :
    print(" {}             | {}".format(clusters20_20[sorted20_20[19-i]], appearences20_20[sorted20_20[19-i]]))

"""#### 25 clusters"""

som20.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som20.clusters)

clusters20_25, neurons_in_clusters20_25 = np.unique(som20.clusters, return_inverse=True)

print(clusters20_25.shape)
print(neurons_in_clusters20_25.shape)

appearences20_25 = np.bincount(neurons_in_clusters20_25)
sorted20_25 = np.argsort(appearences20_25)
print("Clusters sorted by increasing number of neurons:")
print("Cluster index  | Number of neurons ")
print("---------------------------------------------------")
for i in range(len(clusters20_25)):
  if (clusters20_25[sorted20_25[24-i]]>9):
    print(" {}            | {}".format(clusters20_25[sorted20_25[24-i]], appearences20_25[sorted20_25[24-i]]))
  else :
    print(" {}             | {}".format(clusters20_25[sorted20_25[24-i]], appearences20_25[sorted20_25[24-i]]))

"""Το μεγαλύτερο μέγεθος πλέγματος μας δίνει περισσότερο ξεκάθαρες περιοχές στο χάρτη SOM. 

Το clustering συνεχίζει να σέβεται την κατηγοριοποίηση του SOM. Όπως σημειώθηκε παραπάνω, οι συνοριακές (κόκκινες) περιοχές είναι αυτές που συγκεντρώνουν την έντονη εναλλαγή των clusters.

Τέλος, σε σχέση με το μικρότερο μέγεθος πλέγματος, βλεπουμε σημαντική διαφορά στον χάρτη SOM (οι μπλε και κόκκινες περιοχές έχουν μετακινηθεί).

### **25x25**

#### 20 clusters
"""

som25.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som25.clusters)

clusters25_20, neurons_in_clusters25_20 = np.unique(som25.clusters, return_inverse=True)

print(clusters25_20.shape)
print(neurons_in_clusters25_20.shape)

appearences25_20 = np.bincount(neurons_in_clusters25_20)
sorted25_20 = np.argsort(appearences25_20)
print("Clusters sorted by increasing number of neurons:")
print("Cluster index  | Number of neurons ")
print("---------------------------------------------------")
for i in range(len(clusters25_20)):
  if (clusters25_20[sorted25_20[19-i]]>9):
    print(" {}            | {}".format(clusters25_20[sorted25_20[19-i]], appearences25_20[sorted25_20[19-i]]))
  else :
    print(" {}             | {}".format(clusters25_20[sorted25_20[19-i]], appearences25_20[sorted25_20[19-i]]))

"""#### 25 clusters"""

som25.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som25.clusters)

clusters25_25, neurons_in_clusters25_25 = np.unique(som25.clusters, return_inverse=True)

print(clusters25_25.shape)
print(neurons_in_clusters25_25.shape)

appearences25_25 = np.bincount(neurons_in_clusters25_25)
sorted25_25 = np.argsort(appearences25_25)
print("Clusters sorted by increasing number of neurons:")
print("Cluster index  | Number of neurons ")
print("---------------------------------------------------")
for i in range(len(clusters25_25)):
  if (clusters25_25[sorted25_25[24-i]]>9):
    print(" {}            | {}".format(clusters25_25[sorted25_25[24-i]], appearences25_25[sorted25_25[24-i]]))
  else :
    print(" {}             | {}".format(clusters25_25[sorted25_25[24-i]], appearences25_25[sorted25_25[24-i]]))

"""Παρατηρούμε ότι όσο αυξάνουμε το μέγεθος του πλέγματος, ο χάρτης SOM γίνεται ακόμα πιο ξεκάθαρος. Αρχίζει, όμως, να γίνεται κατανοητή η ανάγκη για περισσότερα clusters. Ο χάρτης SOM δίνει διαφορετικές περιοχές, ενώ τα clusters δεν μπορούν να τις διακρίνουν. Με άλλα λόγια, ο χάρτης αποκτά δυνατότητα διαχωρισμού υποκατηγοριών που τα clusters δεν μπορούν να ακολουθήσουν.

### **30x30**

#### 25 clusters
"""

som30.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som30.clusters)

clusters30_25, neurons_in_clusters30_25 = np.unique(som30.clusters, return_inverse=True)

print(clusters30_25.shape)
print(neurons_in_clusters30_25.shape)

appearences30_25 = np.bincount(neurons_in_clusters30_25)
sorted30_25 = np.argsort(appearences30_25)
print("Clusters sorted by increasing number of neurons:")
print("Cluster index  | Number of neurons ")
print("---------------------------------------------------")
for i in range(len(clusters30_25)):
  if (clusters30_25[sorted30_25[24-i]]>9):
    print(" {}            | {}".format(clusters30_25[sorted30_25[24-i]], appearences30_25[sorted30_25[24-i]]))
  else :
    print(" {}             | {}".format(clusters30_25[sorted30_25[24-i]], appearences30_25[sorted30_25[24-i]]))

"""#### 30 clusters"""

som30.view_umatrix(bestmatches=True, colorbar=True, figsize=(15, 15))

print(som30.clusters)

clusters30_30, neurons_in_clusters30_30 = np.unique(som30.clusters, return_inverse=True)

print(clusters30_30.shape)
print(neurons_in_clusters30_30.shape)

appearences30_30 = np.bincount(neurons_in_clusters30_30)
sorted30_30 = np.argsort(appearences30_30)
print("Clusters sorted by increasing number of neurons:")
print("Cluster index  | Number of neurons ")
print("---------------------------------------------------")
for i in range(len(clusters30_30)):
  if (clusters30_30[sorted30_30[29-i]]>9):
    print(" {}            | {}".format(clusters30_30[sorted30_30[29-i]], appearences30_30[sorted30_30[29-i]]))
  else :
    print(" {}             | {}".format(clusters30_30[sorted30_30[29-i]], appearences30_30[sorted30_30[29-i]]))

"""Στην περίπτωση αυτή, κινηθήκαμε σε μεγέθη clusters 25, 30 γιατί τα 20 ήταν πολύ λίγα. Παρατηρώντας ακόμα και την περίπτωση των 25 clusters βλέπουμε να υπάρχει σύγχυση κατηγοριών σχετικά ευδιάκριτων μέσω του χάρτη SOM. Σημειώνουμε ότι η σύγχυση γίνεται στις όχι τόσο έντονα μπλε ή κόκκινες περιοχές.

Συγκρίνοντας τα διάφορα μεγέθη χαρτών, βλέπουμε ότι αλλάζουν οι περιοχές που είναι πιο έντονα κόκκινες ή μπλε. Αυτό είναι λογικό, καθώς όσο δίνουμε τη δυνατότητα για επιπλέον υποκατηγοριοποίηση, σπάμε μεγάλες κατηγορίες με αποτέλεσμα άλλες να είναι τώρα αυτές με τα περισσότερα στοιχεία.

## Σημασιολογική ερμηνεία των clusters

Προκειμένου να μελετήσουμε τις τοπολογικές ιδιότητες του SOM και το αν έχουν ενσωματώσει σημασιολογική πληροφορία για τις ταινίες διαμέσου της διανυσματικής αναπαράστασης με το tf-idf και των κατηγοριών, χρειαζόμαστε ένα κριτήριο ποιοτικής επισκόπησης των clusters. Θα υλοποιήσουμε το εξής κριτήριο: Λαμβάνουμε όρισμα έναν αριθμό (ετικέτα) cluster. Για το cluster αυτό βρίσκουμε όλους τους νευρώνες που του έχουν ανατεθεί από τον k-Means. Για όλους τους νευρώνες αυτούς βρίσκουμε όλες τις ταινίες που τους έχουν ανατεθεί (για τις οποίες αποτελούν bmus). Για όλες αυτές τις ταινίες τυπώνουμε ταξινομημένη τη συνολική στατιστική όλων των ειδών (κατηγοριών) και τις συχνότητές τους. Αν το cluster διαθέτει καλή συνοχή και εξειδίκευση, θα πρέπει κάποιες κατηγορίες να έχουν σαφώς μεγαλύτερη συχνότητα από τις υπόλοιπες. Θα μπορούμε τότε να αναθέσουμε αυτήν/ές την/τις κατηγορία/ες ως ετικέτες κινηματογραφικού είδους στο cluster.

Μπορείτε να υλοποιήσετε τη συνάρτηση αυτή όπως θέλετε. Μια πιθανή διαδικασία θα μπορούσε να είναι η ακόλουθη:

1. Ορίζουμε συνάρτηση `print_categories_stats` που δέχεται ως είσοδο λίστα με ids ταινιών. Δημιουργούμε μια κενή λίστα συνολικών κατηγοριών. Στη συνέχεια, για κάθε ταινία επεξεργαζόμαστε το string `categories` ως εξής: δημιουργούμε μια λίστα διαχωρίζοντας το string κατάλληλα με την `split` και αφαιρούμε τα whitespaces μεταξύ ετικετών με την `strip`. Προσθέτουμε τη λίστα αυτή στη συνολική λίστα κατηγοριών με την `extend`. Τέλος χρησιμοποιούμε πάλι την `np.unique` για να μετρήσουμε συχνότητα μοναδικών ετικετών κατηγοριών και ταξινομούμε με την `np.argsort`. Τυπώνουμε τις κατηγορίες και τις συχνότητες εμφάνισης ταξινομημένα. Χρήσιμες μπορεί να σας φανούν και οι `np.ravel`, `np.nditer`, `np.array2string` και `zip`.

2. Ορίζουμε τη βασική μας συνάρτηση `print_cluster_neurons_movies_report` που δέχεται ως όρισμα τον αριθμό ενός cluster. Με τη χρήση της `np.where` μπορούμε να βρούμε τις συντεταγμένες των bmus που αντιστοιχούν στο cluster και με την `column_stack` να φτιάξουμε έναν πίνακα bmus για το cluster. Προσοχή στη σειρά (στήλη - σειρά) στον πίνακα bmus. Για κάθε bmu αυτού του πίνακα ελέγχουμε αν υπάρχει στον πίνακα μοναδικών bmus που έχουμε υπολογίσει στην αρχή συνολικά και αν ναι προσθέτουμε το αντίστοιχο index του νευρώνα σε μια λίστα. Χρήσιμες μπορεί να είναι και οι `np.rollaxis`, `np.append`, `np.asscalar`. Επίσης πιθανώς να πρέπει να υλοποιήσετε ένα κριτήριο ομοιότητας μεταξύ ενός bmu και ενός μοναδικού bmu από τον αρχικό πίνακα bmus.

3. Υλοποιούμε μια βοηθητική συνάρτηση `neuron_movies_report`. Λαμβάνει ένα σύνολο νευρώνων από την `print_cluster_neurons_movies_report` και μέσω της `indices` φτιάχνει μια λίστα με το σύνολο ταινιών που ανήκουν σε αυτούς τους νευρώνες. Στο τέλος καλεί με αυτή τη λίστα την `print_categories_stats` που τυπώνει τις στατιστικές των κατηγοριών.

Μπορείτε βέβαια να προσθέσετε οποιαδήποτε επιπλέον έξοδο σας βοηθάει. Μια χρήσιμη έξοδος είναι πόσοι νευρώνες ανήκουν στο cluster και σε πόσους και ποιους από αυτούς έχουν ανατεθεί ταινίες.

Θα επιτελούμε τη σημασιολογική ερμηνεία του χάρτη καλώντας την `print_cluster_neurons_movies_report` με τον αριθμός ενός cluster που μας ενδιαφέρει. 

Παράδειγμα εξόδου για ένα cluster (μη βελτιστοποιημένος χάρτης, ωστόσο βλέπετε ότι οι μεγάλες κατηγορίες έχουν σημασιολογική  συνάφεια):

```
Overall Cluster Genres stats:  
[('"Horror"', 86), ('"Science Fiction"', 24), ('"B-movie"', 16), ('"Monster movie"', 10), ('"Creature Film"', 10), ('"Indie"', 9), ('"Zombie Film"', 9), ('"Slasher"', 8), ('"World cinema"', 8), ('"Sci-Fi Horror"', 7), ('"Natural horror films"', 6), ('"Supernatural"', 6), ('"Thriller"', 6), ('"Cult"', 5), ('"Black-and-white"', 5), ('"Japanese Movies"', 4), ('"Short Film"', 3), ('"Drama"', 3), ('"Psychological thriller"', 3), ('"Crime Fiction"', 3), ('"Monster"', 3), ('"Comedy"', 2), ('"Western"', 2), ('"Horror Comedy"', 2), ('"Archaeology"', 2), ('"Alien Film"', 2), ('"Teen"', 2), ('"Mystery"', 2), ('"Adventure"', 2), ('"Comedy film"', 2), ('"Combat Films"', 1), ('"Chinese Movies"', 1), ('"Action/Adventure"', 1), ('"Gothic Film"', 1), ('"Costume drama"', 1), ('"Disaster"', 1), ('"Docudrama"', 1), ('"Film adaptation"', 1), ('"Film noir"', 1), ('"Parody"', 1), ('"Period piece"', 1), ('"Action"', 1)]```
"""

import numpy as np

def print_categories_stats(movie_ids):
  
  all_categories = []
  for id in movie_ids:
    l = [ct.strip() for ct in categories[id][0].split(',')]
    all_categories.extend(l)
  all_categories, freqs = np.unique(all_categories, return_counts=True)
  
  sorted = np.argsort(freqs)
 
  print_list=[]
  for i in sorted[::-1]:
    print_list.append((all_categories[i], freqs[i]))
  print(print_list)

def print_cluster_neurons_movies_report(cluster):
  
  #getting cluster's coordinates
  x, y = np.where(som.clusters==cluster)
  #creating the bmus array
  cluster_bmus = np.column_stack((y, x))

  common_bmus = []
  for i in range(len(ubmus)):
    for j in cluster_bmus:
      if ubmus[i][0]==j[0] and ubmus[i][1]==j[1]:
        common_bmus.append(i)

  return common_bmus

#doesnt say to have parameter cluster but it's convenient...
def neuron_movies_report(cluster):

  movies_indexes = print_cluster_neurons_movies_report(cluster)

  movies = []
  for i in movies_indexes:
    r, = np.where(indices == i)
    movies.extend(list(r))
  
  print_categories_stats(movies)

#see the map, cluster 23, 5 are in blue region meaning they must represent a category
som = som30
ubmus = ubmus30
indices = indices30
neuron_movies_report(23)
neuron_movies_report(0)

#whereas 2, 14 are at a red region => undecisive results
neuron_movies_report(2)
neuron_movies_report(14)

"""## Tips για το SOM και το clustering

- Για την ομαδοποίηση ένα U-matrix καλό είναι να εμφανίζει και μπλε-πράσινες περιοχές (clusters) και κόκκινες περιοχές (ορίων). Παρατηρήστε ποια σχέση υπάρχει μεταξύ αριθμού ταινιών στο final set, μεγέθους grid και ποιότητας U-matrix. ???
- Για το k του k-Means προσπαθήστε να προσεγγίζει σχετικά τα clusters του U-matrix (όπως είπαμε είναι διαφορετικοί μέθοδοι clustering). Μικρός αριθμός k δεν θα σέβεται τα όρια. Μεγάλος αριθμός θα δημιουργεί υπο-clusters εντός των clusters που φαίνονται στο U-matrix. Το τελευταίο δεν είναι απαραίτητα κακό, αλλά μεγαλώνει τον αριθμό clusters που πρέπει να αναλυθούν σημασιολογικά.
- Σε μικρούς χάρτες και με μικρά final sets δοκιμάστε διαφορετικές παραμέτρους για την εκπαίδευση του SOM. Σημειώστε τυχόν παραμέτρους που επηρεάζουν την ποιότητα του clustering για το dataset σας ώστε να τις εφαρμόσετε στους μεγάλους χάρτες.
- Κάποια τοπολογικά χαρακτηριστικά εμφανίζονται ήδη σε μικρούς χάρτες. Κάποια άλλα χρειάζονται μεγαλύτερους χάρτες. Δοκιμάστε μεγέθη 20x20, 25x25 ή και 30x30 και αντίστοιχη προσαρμογή των k. Όσο μεγαλώνουν οι χάρτες, μεγαλώνει η ανάλυση του χάρτη αλλά μεγαλώνει και ο αριθμός clusters που πρέπει να αναλυθούν.

## Ανάλυση τοπολογικών ιδιοτήτων χάρτη SOM

Μετά το πέρας της εκπαίδευσης και του clustering θα έχετε ένα χάρτη με τοπολογικές ιδιότητες ως προς τα είδη των ταίνιών της συλλογής σας, κάτι αντίστοιχο με την εικόνα στην αρχή της Εφαρμογής 2 αυτού του notebook (η συγκεκριμένη εικόνα είναι μόνο για εικονογράφιση, δεν έχει καμία σχέση με τη συλλογή δεδομένων και τις κατηγορίες μας).

Για τον τελικό χάρτη SOM που θα παράξετε για τη συλλογή σας, αναλύστε σε markdown με συγκεκριμένη αναφορά σε αριθμούς clusters και τη σημασιολογική ερμηνεία τους τις εξής τρεις τοπολογικές ιδιότητες του SOM: 

1. Δεδομένα που έχουν μεγαλύτερη πυκνότητα πιθανότητας στο χώρο εισόδου τείνουν να απεικονίζονται με περισσότερους νευρώνες στο χώρο μειωμένης διαστατικότητας. Δώστε παραδείγματα από συχνές και λιγότερο συχνές κατηγορίες ταινιών. Χρησιμοποιήστε τις στατιστικές των κατηγοριών στη συλλογή σας και τον αριθμό κόμβων που χαρακτηρίζουν.
2. Μακρινά πρότυπα εισόδου τείνουν να απεικονίζονται απομακρυσμένα στο χάρτη. Υπάρχουν χαρακτηριστικές κατηγορίες ταινιών που ήδη από μικρούς χάρτες τείνουν να τοποθετούνται σε διαφορετικά ή απομονωμένα σημεία του χάρτη.
3. Κοντινά πρότυπα εισόδου τείνουν να απεικονίζονται κοντά στο χάρτη. Σε μεγάλους χάρτες εντοπίστε είδη ταινιών και κοντινά τους υποείδη.

Προφανώς τοποθέτηση σε 2 διαστάσεις που να σέβεται μια απόλυτη τοπολογία δεν είναι εφικτή, αφενός γιατί δεν υπάρχει κάποια απόλυτη εξ ορισμού για τα κινηματογραφικά είδη ακόμα και σε πολλές διαστάσεις, αφετέρου γιατί πραγματοποιούμε μείωση διαστατικότητας.

Εντοπίστε μεγάλα clusters και μικρά clusters που δεν έχουν σαφή χαρακτηριστικά. Εντοπίστε clusters συγκεκριμένων ειδών που μοιάζουν να μην έχουν τοπολογική συνάφεια με γύρω περιοχές. Προτείνετε πιθανές ερμηνείες.



Τέλος, εντοπίστε clusters που έχουν κατά την άποψή σας ιδιαίτερο ενδιαφέρον στη συλλογή της ομάδας σας (data exploration / discovery value) και σχολιάστε.

Ορίζουμε σαν τελικό χάρτη SOM από εκείνους που δημιουργήσαμε, τον χάρτη με μέγεθος πλέγματος 30. Η εν λόγω εφαρμογή κατηγοριοποιεί εγγενώς τις ταινίες σε πολλά διαφορετικά είδη. Συνεπώς, για να μπορούμε να προβλέψουμε σωστά τα labels θέλουμε να υπάρχουν όσο το δυνατόν περισσότερες περιοχές-clusters. Όπως έχουμε ήδη παρατηρήσει, η μέγαλη ανάλυση σε επιμέρους κατηγορίες γίνεται όσο αυξάνουμε το μέγεθος του πλέγματος του χάρτη SOM.
"""

#1

#pdf of all the categories in dataset 
print_categories_stats(range(5000))

"""Βλέπουμε παραπάνω τις κατηγορίες που εμφανίζονται στο σύνολο δεδομένων κατά σειρά συχνότητας (από τη μεγαλύτερη πυκνότητα πιθανότητας-ποσοστό εμφάνισης στη μικρότερη). Για να επιβεβαιώσουμε την παρατήρηση 1, θα εξετάσουμε τα clustersμε τους περισσότερους και του λιγότερους νευρώνες στο χάρτη της επιλογής μας."""

som = som30
ubmus = ubmus30
indices = indices30
#the first 3 clusters with the most neurons are 13,14 and 9
neuron_movies_report(13)
neuron_movies_report(14)
neuron_movies_report(9)

#the clusters with the less neurons are 17, 25, 0
neuron_movies_report(17)
neuron_movies_report(25)
neuron_movies_report(0)

"""Παρατηρούμε ότι το Drama και το Comedy που έχουν μεγάλη πυκνότητα πιθανότητας στο χώρο εισόδου κυριαρχούν στους νευρώνες, καθώς βλέπουμε ότι απεικονίζεται στο cluster με το μεγαλύτερο πλήθος νευρώνων και μάλιστα έχουν τους περισσότερους νευρώνες σε αυτό .

Αντίστοιχα σε αυτά με το μικρότερο πλήθος νευρώνων συναντάμε είδη όπως το Thriller, Slasher, Chinese Movies τα οποία είναι πολύ σπάνια. Επίσης, σε αυτήν την περίπτωση, τα clusters δεν είναι ξεκάθαρα τοποθετημένα σε μία κατηγορία. Δηλαδή, τόσο το Slasher όσο και το Horror έχουν 77 νευρώνες στο cluster 25.
"""

#2
som = som10
ubmus = ubmus10
indices = indices10
#choosing clusters that are far on the map to find categories that tend to be apart 
neuron_movies_report(10)
neuron_movies_report(9)
#indeed Documentary and Action films are very different
#however notice Comedy and Comedy film!

"""Επιλέγουμε στον μικρότερο χάρτη (μέγεθος πλέγματος 10) clusters σε δύο απομακρυσμένες περιοχές του (πάνω δεξιά και κάτω αριστερά). Τα δύο αυτά clusters χαρακτηρίζονται ως Documentary και Action/Adventure. Παρατηρούμε ότι τα δύο αυτά είδη είναι σημασιολογικά πολύ διαφορετικά. Άρα, αυτός ο διαχωρισμός είναι λογικός και μας ενθαρρύνει σχετικά με τη λειτουργία του συστήματός μας."""

som = som20
ubmus = ubmus20
indices = indices20
neuron_movies_report(16)
neuron_movies_report(7)

"""Ελέγχουμε απομακρυσμένες στο χάρτη περιοχές για λίγο μεγαλύτερο μέγεθος πλέγματος (20). Παρατηρούμε ότι και πάλι τα δύο αυτά είδη διαχωρίζονται (εδώ το Documentary δε χαρακτηρίζει το cluster αλλά αυτό είναι λογικό, καθώς είναι πολύ συγκεκριμένο είδος ταινίας)."""

#3: finding similar kinds of movies (they are close on the map)
som = som30
ubmus = ubmus30
indices = indices30
neuron_movies_report(25)
neuron_movies_report(5)

"""Σε αυτή την περίπτωση δουλεύουμε τον μεγαλύτερο χάρτη που είναι και αυτός που επιλέξαμε ως βέλτιστο. Παρατηρούμε την πάνω αριστερή γωνία του χάρτη. Υπάρχει μία μεγάλη πράσινη περιοχή που φαίνεται να ανήκει σε μία κατηγορία. Ωστόσο, μία δεύτερη κατηγορία φαίνεται να εμφανίζεται αχνά και μία δεύτερη, πιθανώς υποκατηγορία. O k-Means "καταλαβαίνει" την ύπαρξή της και γι'αυτό δημιουργεί δύο clusters στην περιοχή. Πράγματι, τα δύο είδη Slasher και Horror είναι πολύ κοντά μεταξύ τους όπως ήταν αναμενόμενο. """

#big cluster with unclear characteristics 

neuron_movies_report(14)

"""Το cluster 14 είναι το δεύτερο μεγαλύτερο στο χάρτη. Ωστόσο, παρατηρούμε ότι οι κατηγορίες που εμφανίζονται σε ταξινομημένη σειρά είναι πολύ κοντά μεταξύ τους έχοντας η κάθε μια λίγες αναθέσεις. Επιπλέον, οι πρώτες κατηγορίες είναι Black-and-White και Japanese films. Οι δύο αυτές κατηγορίες δεν έχουν απαραίτητα κάτι κοινό μεταξύ τους και έτσι το περιεχόμενο του συγκεκριμένου cluster παραμένει αρκετά αβέβαιο. """

#small cluster with unclear characteristics 

neuron_movies_report(24)

"""Το cluster 24 περιέχει 15 νευρώνες, συνεπώς είναι από τα μικρότερα του χάρτη. Παρατηρούμε ότι το είδος της ταινίας σε αυτό δεν είναι σαφώς καθορισμένο. Οι κατηγορίες Romance, Drama, Comedy, αν και πολύ διαορετικές μεταξύ τους ως είδη, κυριαρχούν εξίσου στο cluster. """

#clusters which are near but they have a much different category

neuron_movies_report(2)
neuron_movies_report(4)

"""Τα δύο παραπάνω clusters είναι τοποθετημένα δίπλα στο χάρτη SOM. Ωστόσο, παρατηρούμε ότι διαφέρουν σημαντικά ως είδη, αφού το ένα ανήκει στην κατηγορία Drama-Romantic drama ενώ το άλλο είναι σχετικό με Science Fiction. Φυσικά, ταινίες Science Fiction μπορεί να έχουν Romance αλλά δεν είναι επιθυμητό από ένα σύστημα συστάσεων να προτείνει μία καθαρά ρομαντική ταινία σε κάποιον που έχει παρακολουθήσει απλώς μία ταινία επιστημονικής φαντασίας."""

neuron_movies_report(23)

"""Το cluster 23 χαρακτηρίζεται ως είδος Western. Κατ' αρχάς θεωρούμε ιδιαίτερα εντυπωσιακό το ότι το σύστημα που δημιουργήσαμε μπορεί να χαρακτηρίζει τόσο συγκεκριμένες κατηγορίες. Αν και σκεφτόμενοι την περιγραφή μίας ταινίας Western (έντονο ειδικό λεξιλόγιο), το γεγονός θα μας κάνει λιγότερη εντύπωση. 

Σημειώνουμε ότι το εν λόγω cluster εμφανίζεται σε δύο σημεία του χάρτη SOM. Η παρατήρηση αυτή δηλώνει ότι, ως προς αυτό το είδος, ο k-Means εχει πολύ καλή διακριτική ικανότητα αφού μπορεί να το εντοπίζει ξεκάθαρα.

# Τελική παράδοση άσκησης

- Θα παραδώσετε στο eclass το παρόν notebook επεξεργασμένο ή ένα νέο με τις απαντήσεις σας για τα ζητούμενα και των δύο εφαρμογών. 
- Θυμηθείτε ότι η ανάλυση του χάρτη στο markdown με αναφορά σε αριθμούς clusters πρέπει να αναφέρεται στον τελικό χάρτη με τα κελιά ορατά που θα παραδώσετε αλλιώς ο χάρτης που θα προκύψει θα είναι διαφορετικός και τα labels των clusters δεν θα αντιστοιχούν στην ανάλυσή σας. 
- Μην ξεχάσετε στην αρχή ένα κελί markdown με **τα στοιχεία της ομάδας σας**.
- Στο **zip** που θα παραδώσετε πρέπει να βρίσκονται **2 αρχεία (το .ipynb και το .py του notebook σας)**.

<table>
  <tr><td align="center">
    <font size="4">Παρακαλούμε διατρέξτε βήμα-βήμα το notebook για να μην ξεχάσετε παραδοτέα!</font>
</td>
  </tr>
</table>
"""